{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d079b2c0",
   "metadata": {},
   "source": [
    "## **3. Attivazione e Gradiente MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d678d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c3f52fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('./data/names.txt', 'r').read().splitlines()\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88b17eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabolario di caratteri e mapping a interi\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0138fc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# costruiamo il dataset\n",
    "def build_dataset(words, n):\n",
    "    block_size = n\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        #print(w)\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            #print(''.join(itos[i] for i in context), '-->', itos[ix])\n",
    "            context = context[1:] + [ix]\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "block_size = 3\n",
    "X_train, y_train = build_dataset(words[:n1], block_size)\n",
    "X_valid, y_valid = build_dataset(words[n1:n2], block_size)\n",
    "X_test, y_test = build_dataset(words[n2:], block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70e824ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP sistemato:\n",
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "\n",
    "C = torch.randn((vocab_size, n_embd))\n",
    "W1 = torch.randn((n_embd*block_size, n_hidden))\n",
    "b1 = torch.randn(n_hidden)\n",
    "W2 = torch.randn((n_hidden, vocab_size))\n",
    "b2 = torch.randn((vocab_size))\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1eff27ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9279758930206299\n"
     ]
    }
   ],
   "source": [
    "# Training:\n",
    "batch_size = 32\n",
    "for i in range(80000):\n",
    "    # costruzione minibatch\n",
    "    ix = torch.randint(0, X_train.shape[0], (batch_size,)) \n",
    "    Xb, yb = X_train[ix], y_train[ix]\n",
    "    # Forward pass:\n",
    "    emb = C[Xb] # embedding dei caratteri in un vettore\n",
    "    h = torch.tanh(emb.view(emb.shape[0], n_embd*block_size) @ W1 + b1) # hidden layer\n",
    "    logits = h @ W2 + b2 # output layer\n",
    "    loss = F.cross_entropy(logits, yb) # loss function\n",
    "    # Backward pass:\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # Update:\n",
    "    lr = 0.1 if i < 50000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1019cf9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.200448751449585\n",
      "val 2.2250044345855713\n",
      "test 2.228882312774658\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad() # disabilita il tracciatore al gradiente\n",
    "def split_loss(split):\n",
    "    x,y = {\n",
    "        'train': (X_train, y_train),\n",
    "        'val': (X_valid, y_valid),\n",
    "        'test': (X_test, y_test),\n",
    "    }[split]\n",
    "    emb = C[x]\n",
    "    h = torch.tanh(emb.view(emb.shape[0], n_embd*block_size) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "    \n",
    "split_loss('train')\n",
    "split_loss('val')\n",
    "split_loss('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c608bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kely.\n",
      "tiel.\n",
      "makessotarrionna.\n",
      "shaadvith.\n",
      "rithjoseyn.\n",
      "lanyret.\n",
      "julie.\n",
      "yogsaet.\n",
      "izoriah.\n",
      "aylyn.\n"
     ]
    }
   ],
   "source": [
    "# Genero nomi dal modello:\n",
    "for i in range(10):\n",
    "    out = []\n",
    "    context = [0] * block_size\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])]\n",
    "        h = torch.tanh(emb.view(emb.shape[0], n_embd*block_size) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1, replacement=True).item() # campionamente per ottenere successore\n",
    "        out.append(ix)\n",
    "        context = context[1:] + [ix]\n",
    "        if (ix==0): # se è 0 vuol dire che la parole è terminata\n",
    "            break\n",
    "\n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8bc558e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La nostra rete non è configurata correttamente nell'inizializzazione:\n",
    "# all'inizio ci aspetteremo che la rete sia configurata in modo tale che ogni carattere abbia la stessa probabilità di essere il prossimo in quanto non abbiamo dati su cui basarci\n",
    "1 / 27.0\n",
    "# se controlliamo la loss function della rete con queste caratteristiche otteniamo \n",
    "-torch.tensor(1/27.0).log()\n",
    "# che è molto minore a quello che abbiamo noi, questo perchè la rete all'inizio da pesi casuali a ogni carattere quindi può capitare che la rete sia erroneamente molto confidente del fatto che un carattere possa essere il prossimo\n",
    "# vogliamo quindi rendere simili i valori di logits = h @ W2 + b2, possiamo fare ciò diminuendo il valore dei parametri iniziali:\n",
    "W2 = torch.randn((n_hidden, vocab_size)) * 0.01\n",
    "b2 = torch.randn((vocab_size)) * 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f67782fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ora che abbiamo sistemato l'aspetto dei pesi causali iniziali, possiamo notare la presenza di un altro problema:\n",
    "# usare tanh come funzione di attivazione rende il gradiente da backpropagare non rilevante quando il valore di tanh è vicino ad 1 o -1 ovvero quando siamo vicini alle code\n",
    "# quindi cambiare gli input in questo caso non impatterà l'output di tanh dato che è in una regione non rilevante\n",
    "# questo significa che i bias e pesi corrispondenti non impattano la loss\n",
    "# Nel caso in cui un neurone avesse tutti i valori vicini alle code, esso è un neurone morto e quindi non importa che input prenda, esso non imparerà niente -> NON VOGLIAMO CHE QUESTO ACCADA\n",
    "\n",
    "# In questo momento la nostra rete ha i neuroni nello strato nascosto con molti valori vicini alle code all'inizializzazione, ma nessuno di essi è morto; quindi non abbiamo grossi problemi\n",
    "# Nonostante ciò preferiamo diminuire i valori estremi all'inizializzazione in modo tale che la rete neurale sia più reattiva: vogliamo quindi che il valore in input alla funzione di attivazione nello strato nascosto sia più vicino a 0\n",
    "C = torch.randn((vocab_size, n_embd))\n",
    "W1 = torch.randn((n_embd*block_size, n_hidden)) * (5/3)/((n_embd*block_size)**0.5) # questo serve per rendere la deviazione standard = 1, il gain 5/3 è soggettivo per il tanh, ogni funzione di attivazione ha il proprio gain\n",
    "b1 = torch.randn(n_hidden) * 0.01\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131ddb63",
   "metadata": {},
   "source": [
    "## **Normalizzazione Batch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986f0c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una inizializzazione precisa delle configurazioni iniziali non è fondamentale al giorno d'oggi.\n",
    "# Questo perchè ci sono delle innovazioni che permettono di ottimizzare il modello.\n",
    "# Nello scorso blocco abbiamo detto che abbiamo bisogno di normalizzare la distribuzione delle funzioni di attivazione nell'hidden layer (quindi media 0 e sd 1)\n",
    "# Per raggiungere questo obiettivo invece di scalare i pesi e i bias, possiamo semplicemente normalizzarli una volta calcolato l'input della funzione di attivazione -> questa tecnica è chiamata batch normalization:\n",
    "\n",
    "bn_gain = torch.ones((1, n_hidden))\n",
    "bn_bias = torch.zeros((1, n_hidden))\n",
    "parameters = [C, W1, b1, W2, b2, bn_gain, bn_bias]\n",
    "batch_size = 32\n",
    "for i in range(80000):\n",
    "    # costruzione minibatch\n",
    "    ix = torch.randint(0, X_train.shape[0], (batch_size,)) \n",
    "    Xb, yb = X_train[ix], y_train[ix]\n",
    "    # Forward pass:\n",
    "    emb = C[Xb] # embedding dei caratteri in un vettore\n",
    "    h_inact = emb.view(emb.shape[0], n_embd*block_size) @ W1 + b1 # input della funzione di attivazione, quello che vogliamo fare in questo caso è normalizzarlo\n",
    "    h_inact = bn_gain * (h_inact-h_inact.mean(0, keepdim=True)) / h_inact.std(0, keepdim=True) + bn_bias # batch notmalization e aggiunta di un gain e bias che all'inizio sono 1 e 0 e quindi non influiscono nella computazione ma durante l'ottimizzazione, tramite back propagation variano da neurone a neurone\n",
    "    h = torch.tanh(h_inact) # hidden layer\n",
    "    logits = h @ W2 + b2 # output layer\n",
    "    loss = F.cross_entropy(logits, yb) # loss function\n",
    "    # Backward pass:\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # Update:\n",
    "    lr = 0.1 if i < 50000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "print(loss.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
